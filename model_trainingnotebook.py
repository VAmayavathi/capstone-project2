# -*- coding: utf-8 -*-
"""final of imagecorrectness.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1dYRLq2l9oZ-Xnmmq84Lk0l-gNaR9lAT5
"""

!pip install pytesseract

!apt-get install -y tesseract-ocr
!pip install pytesseract

from google.colab import drive
drive.mount('/content/drive')

import os
os.listdir('/content/drive/MyDrive/')

!mv "/content/drive/MyDrive/Colab Notebooks/image_correctness_datasets" "/content/drive/MyDrive/"

import tensorflow as tf
from tensorflow.keras.preprocessing.image import ImageDataGenerator
from tensorflow.keras.applications import EfficientNetB0
from tensorflow.keras.models import Model
from tensorflow.keras.layers import GlobalAveragePooling2D, Dense, Dropout
from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau

BASE_DIR = "/content/drive/MyDrive/image_correctness_datasets/Merged"
TRAIN_DIR = os.path.join(BASE_DIR, "train")
VAL_DIR = os.path.join(BASE_DIR, "val")
TEST_DIR = os.path.join(BASE_DIR, "test")

IMG_SIZE = (224, 224)
BATCH_SIZE = 16
EPOCHS = 10
NUM_CLASSES = 1  # Binary classification (original or tampered)

train_datagen = ImageDataGenerator(
    rescale=1./255,
    rotation_range=15,
    width_shift_range=0.1,
    height_shift_range=0.1,
    shear_range=0.1,
    zoom_range=0.1,
    horizontal_flip=True
)

val_test_datagen = ImageDataGenerator(rescale=1./255)

train_generator = train_datagen.flow_from_directory(
    TRAIN_DIR, target_size=IMG_SIZE, batch_size=BATCH_SIZE, class_mode="binary"
)

val_generator = val_test_datagen.flow_from_directory(
    VAL_DIR, target_size=IMG_SIZE, batch_size=BATCH_SIZE, class_mode="binary"
)

test_generator = val_test_datagen.flow_from_directory(
    TEST_DIR, target_size=IMG_SIZE, batch_size=BATCH_SIZE, class_mode="binary"
)

base_model = EfficientNetB0(weights="imagenet", include_top=False, input_shape=(224, 224, 3))
base_model.trainable = False

x = GlobalAveragePooling2D()(base_model.output)
x = Dense(128, activation="relu")(x)
x = Dropout(0.3)(x)
output = Dense(1, activation="sigmoid", dtype=tf.float32)(x)

model = Model(inputs=base_model.input, outputs=output)

model.compile(
    optimizer=tf.keras.optimizers.Adam(learning_rate=0.0001),
    loss="binary_crossentropy",
    metrics=["accuracy"]
)

model.summary()

early_stopping = EarlyStopping(monitor="val_loss", patience=3, restore_best_weights=True)
reduce_lr = ReduceLROnPlateau(monitor="val_loss", factor=0.5, patience=2, verbose=1)

history = model.fit(
    train_generator,
    validation_data=val_generator,
    epochs=EPOCHS,
    callbacks=[early_stopping, reduce_lr]
)

import zipfile
import os
import shutil

base_dir = '/content/drive/MyDrive/image_correctness_datasets/Merged'
temp_zip_path = '/content/image_correctness_dataset.zip'  # üí° Local Colab storage (RAM/disk)
final_zip_path = '/content/drive/MyDrive/image_correctness_datasets/image_correctness_dataset.zip'

folders_to_include = ['train', 'val', 'test']

# üî• Zipping faster in Colab local storage
with zipfile.ZipFile(temp_zip_path, 'w', zipfile.ZIP_DEFLATED) as zipf:
    for folder in folders_to_include:
        folder_path = os.path.join(base_dir, folder)
        for root, _, files in os.walk(folder_path):
            for file in files:
                file_path = os.path.join(root, file)
                arcname = os.path.relpath(file_path, base_dir)
                zipf.write(file_path, arcname)

# ‚úÖ Move the zip to Drive after it's ready
shutil.move(temp_zip_path, final_zip_path)

print("‚úÖ Fast zip created and moved to Drive:", final_zip_path)

test_loss, test_acc = model.evaluate(test_generator)
print(f"‚úÖ Test Accuracy: {test_acc:.2%}")

model.save("/content/drive/MyDrive/image_correctness_datasets/efficientnet_model.keras")
print("‚úÖ Model Saved Successfully in Keras format!")

model = tf.keras.models.load_model("/content/drive/MyDrive/image_correctness_datasets/efficientnet_model.keras")
print("‚úÖ Model Loaded Successfully!")

from tensorflow.keras.preprocessing import image
import numpy as np

img_path = '/content/drive/MyDrive/your_image.jpg'  # Provide path to your test image
img = image.load_img(img_path, target_size=(224, 224))
img_array = image.img_to_array(img) / 255.0
img_array = np.expand_dims(img_array, axis=0)

prediction = model.predict(img_array)
print(f"Prediction: {prediction[0][0]:.2%} (1 = Tampered, 0 = Original)")





import os

dataset_path = "/content/drive/MyDrive/image_correctness_datasets"  # Update this path
files = os.listdir(dataset_path)

print(f"Total files: {len(files)}")
print("Sample files:", files[:10])  # Show first 10 files

casia_path = os.path.join(dataset_path, "casia_v2")

if os.path.exists(casia_path):
    print("CASIA v2 contains:", os.listdir(casia_path))
else:
    print("Error: CASIA v2 folder not found.")

psb_path = os.path.join(dataset_path, "PS-Battles")

if os.path.exists(psb_path):
    print("PS-Battles contains:", os.listdir(psb_path))
else:
    print("Error: PS-Battles folder not found.")

import cv2
import numpy as np
import matplotlib.pyplot as plt

# Define paths
correct_path = os.path.join(casia_path, "correct")
incorrect_path = os.path.join(casia_path, "incorrect")

# Function to load and preprocess images
def load_images_from_folder(folder, label, img_size=(224, 224), max_samples=5):
    images = []
    labels = []
    filenames = os.listdir(folder)[:max_samples]  # Load only a few samples for now

    for filename in filenames:
        img_path = os.path.join(folder, filename)
        img = cv2.imread(img_path)  # Read image
        if img is not None:
            img = cv2.resize(img, img_size)  # Resize
            img = img / 255.0  # Normalize
            images.append(img)
            labels.append(label)

    return np.array(images), np.array(labels)

# Load sample images
correct_images, correct_labels = load_images_from_folder(correct_path, label=1)  # 1 for correct
incorrect_images, incorrect_labels = load_images_from_folder(incorrect_path, label=0)  # 0 for incorrect

# Combine datasets
X_sample = np.concatenate((correct_images, incorrect_images), axis=0)
y_sample = np.concatenate((correct_labels, incorrect_labels), axis=0)

# Display some sample images
fig, axes = plt.subplots(1, 5, figsize=(15, 5))
for i in range(5):
    axes[i].imshow(X_sample[i])
    axes[i].set_title(f"Label: {y_sample[i]}")
    axes[i].axis("off")
plt.show()

import pandas as pd

ps_originals = pd.read_csv(os.path.join(psb_path, "originals.tsv"), sep="\t")
ps_photoshops = pd.read_csv(os.path.join(psb_path, "photoshops.tsv"), sep="\t")

print("Originals Sample:")
print(ps_originals.head())

print("\nPhotoshops Sample:")
print(ps_photoshops.head())

from google.colab import drive
drive.mount('/content/drive')

import os
import requests
from PIL import Image
from io import BytesIO

headers = {
    "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36"
}

# Function to download images with headers
def download_images(url_list, save_dir, max_images=5):
    os.makedirs(save_dir, exist_ok=True)
    for i, url in enumerate(url_list[:max_images]):
        try:
            response = requests.get(url, headers=headers, timeout=10)
            if response.status_code == 200:
                img = Image.open(BytesIO(response.content))
                img.save(os.path.join(save_dir, f"image_{i}.jpg"))
                print(f"Downloaded: {url}")
            else:
                print(f"Failed: {url} - Status Code: {response.status_code}")
        except Exception as e:
            print(f"Error downloading {url}: {e}")

# Extract URLs
originals_urls = ps_originals.iloc[:, 1].tolist()  # Adjust column index if needed
photoshops_urls = ps_photoshops.iloc[:, 2].tolist()

# Download images
originals_dir = "/content/drive/MyDrive/image_correctness_dataset/PS-Battles/originals"
photoshops_dir = "/content/drive/MyDrive/image_correctness_dataset/PS-Battles/photoshops"

download_images(originals_urls, originals_dir)
download_images(photoshops_urls, photoshops_dir)

import os

# Paths
originals_dir = "/content/drive/MyDrive/image_correctness_dataset/PS-Battles/originals"
photoshops_dir = "/content/drive/MyDrive/image_correctness_dataset/PS-Battles/photoshops"

# Check files
print(f"Originals: {len(os.listdir(originals_dir))} images")
print(f"Photoshops: {len(os.listdir(photoshops_dir))} images")

import matplotlib.pyplot as plt
from PIL import Image

# Function to display images
def show_images(image_folder, num_images=5):
    files = os.listdir(image_folder)[:num_images]
    fig, axes = plt.subplots(1, len(files), figsize=(15, 5))

    for i, file in enumerate(files):
        img = Image.open(os.path.join(image_folder, file))
        axes[i].imshow(img)
        axes[i].axis("off")

    plt.show()

# Show sample images
print("Originals Sample:")
show_images(originals_dir)

print("Photoshops Sample:")
show_images(photoshops_dir)

!find /content/drive -type d -name "image_correctness_dataset"

import shutil
import os

# Define paths
encrypted_path = "/content/drive/.Encrypted/MyDrive/image_correctness_dataset"
mydrive_path = "/content/drive/MyDrive/image_correctness_dataset"

# Define subfolders
originals_src = os.path.join(encrypted_path, "PS-Battles", "originals")
photoshops_src = os.path.join(encrypted_path, "PS-Battles", "photoshops")

originals_dest = os.path.join(mydrive_path, "PS-Battles", "originals")
photoshops_dest = os.path.join(mydrive_path, "PS-Battles", "photoshops")

# Move images if folders exist
if os.path.exists(originals_src) and os.path.exists(photoshops_src):
    shutil.move(originals_src, originals_dest)
    shutil.move(photoshops_src, photoshops_dest)
    print("‚úÖ Images moved successfully!")
else:
    print("‚ùå Some folders do not exist, check paths!")

import shutil

shutil.rmtree("/content/drive/.Encrypted/MyDrive/image_correctness_dataset", ignore_errors=True)
print("üóëÔ∏è Encrypted dataset folder deleted!")

# Define dataset paths
correct_path = "/content/drive/MyDrive/image_correctness_datasets/PS-Battles/originals"
incorrect_path = "/content/drive/MyDrive/image_correctness_datasets/PS-Battles/photoshops"

# Run preprocessing
X_correct, y_correct = preprocess_images(correct_path)
X_incorrect, y_incorrect = preprocess_images(incorrect_path)

print(f"‚úÖ Processed {len(X_correct)} correct images and {len(X_incorrect)} incorrect images.")

import os

correct_path = "/content/drive/MyDrive/image_correctness_datasets/PS-Battles/originals"
incorrect_path = "/content/drive/MyDrive/image_correctness_datasets/PS-Battles/photoshops"

# Check if directories exist
print("‚úÖ Originals folder exists:", os.path.exists(correct_path))
print("‚úÖ Photoshops folder exists:", os.path.exists(incorrect_path))

# Check if images exist inside these folders
print("üìÇ Images in originals:", os.listdir(correct_path)[:5])  # Show first 5 images
print("üìÇ Images in photoshops:", os.listdir(incorrect_path)[:5])  # Show first 5 images

import os

base_path = "/content/drive/MyDrive/image_correctness_dataset/PS-Battles"

# Find all JPG files in the dataset
jpg_files = []
for root, dirs, files in os.walk(base_path):
    for file in files:
        if file.lower().endswith(".jpg"):
            jpg_files.append(os.path.join(root, file))

print(f"üìÇ Found {len(jpg_files)} images")
print("üñºÔ∏è Sample images:", jpg_files[:5])

!find /content/drive/ -name "*.jpg"

!python3 /content/drive/MyDrive/image_correctness_datasets/PS-Battles/downloader.py

ls -lh /content/drive/MyDrive/image_correctness_datasets/PS-Battles

# Commented out IPython magic to ensure Python compatibility.
# %cd /content/drive/MyDrive/image_correctness_datasets/PS-Battles

# Commented out IPython magic to ensure Python compatibility.
# %cd /content/drive/MyDrive/image_correctness_datasets/PS-Battles

!ls -lh

!python3 downloader.py

import os

originals_path = "/content/drive/MyDrive/image_correctness_datasets/PS-Battles/originals"
photoshops_path = "/content/drive/MyDrive/image_correctness_datasets/PS-Battles/photoshops"

print(f"‚úÖ Originals: {len(os.listdir(originals_path))} images")
print(f"‚úÖ Photoshops: {sum(len(files) for _, _, files in os.walk(photoshops_path))} images")

import os

photoshops_path = "/content/drive/MyDrive/image_correctness_datasets/PS-Battles/photoshops"

folders = [f for f in os.listdir(photoshops_path) if os.path.isdir(os.path.join(photoshops_path, f))]
print(f"üìÇ Found {len(folders)} subfolders inside photoshops")

import pandas as pd

photoshops_tsv = "/content/drive/MyDrive/image_correctness_datasets/PS-Battles/photoshops.tsv"
df = pd.read_csv(photoshops_tsv, sep="\t")

print(f"üìù Total photoshops.tsv entries: {len(df)}")

import os
import shutil

# Paths for original datasets
casia_path = "/content/drive/MyDrive/image_correctness_datasets/casia_v2"
psb_path = "/content/drive/MyDrive/image_correctness_datasets/PS-Battles"

# New merged dataset
merged_path = "/content/drive/MyDrive/image_correctness_datasets/Merged"

# Define source directories
casia_originals = os.path.join(casia_path, "correct")
casia_tampered = os.path.join(casia_path, "incorrect")
psb_originals = os.path.join(psb_path, "originals")
psb_tampered = os.path.join(psb_path, "photoshops")

# Define target directories
merged_originals = os.path.join(merged_path, "originals")
merged_tampered = os.path.join(merged_path, "tampered")

# Create target directories if they don't exist
os.makedirs(merged_originals, exist_ok=True)
os.makedirs(merged_tampered, exist_ok=True)

# Function to copy images
def copy_images(source_folder, target_folder):
    for filename in os.listdir(source_folder):
        source_path = os.path.join(source_folder, filename)
        target_path = os.path.join(target_folder, filename)
        shutil.copy(source_path, target_path)

# Copy original (authentic) images
copy_images(casia_originals, merged_originals)
copy_images(psb_originals, merged_originals)

# Copy tampered (photoshopped) images
copy_images(casia_tampered, merged_tampered)
copy_images(psb_tampered, merged_tampered)

print("‚úÖ Dataset Merging Completed Successfully!")

import os
import shutil

# Paths for original datasets
casia_path = "/content/drive/MyDrive/image_correctness_datasets/casia_v2"
psb_path = "/content/drive/MyDrive/image_correctness_datasets/PS-Battles"

# New merged dataset
merged_path = "/content/drive/MyDrive/image_correctness_datasets/Merged"

# Define source directories
casia_originals = os.path.join(casia_path, "correct")
casia_tampered = os.path.join(casia_path, "incorrect")
psb_originals = os.path.join(psb_path, "originals")
psb_tampered = os.path.join(psb_path, "photoshops")  # This contains subfolders

# Define target directories
merged_originals = os.path.join(merged_path, "originals")
merged_tampered = os.path.join(merged_path, "tampered")

# Create target directories if they don't exist
os.makedirs(merged_originals, exist_ok=True)
os.makedirs(merged_tampered, exist_ok=True)

# Function to copy images (handles subfolders)
def copy_images(source_folder, target_folder):
    for item in os.listdir(source_folder):
        source_path = os.path.join(source_folder, item)
        if os.path.isfile(source_path):  # ‚úÖ Copy only files
            target_path = os.path.join(target_folder, item)
            shutil.copy(source_path, target_path)
        elif os.path.isdir(source_path):  # üìÇ If it's a folder, copy images inside
            for subfile in os.listdir(source_path):
                subfile_path = os.path.join(source_path, subfile)
                if os.path.isfile(subfile_path):  # ‚úÖ Copy only image files inside subfolders
                    target_path = os.path.join(target_folder, subfile)
                    shutil.copy(subfile_path, target_path)

# Copy original (authentic) images
copy_images(casia_originals, merged_originals)
copy_images(psb_originals, merged_originals)

# Copy tampered (photoshopped) images (handling subfolders)
copy_images(casia_tampered, merged_tampered)
copy_images(psb_tampered, merged_tampered)  # ‚úÖ Now correctly handles subfolders

print("‚úÖ Dataset Merging Completed Successfully!")

import os

# Define merged dataset paths
merged_originals = "/content/drive/MyDrive/image_correctness_datasets/Merged/originals"
merged_tampered = "/content/drive/MyDrive/image_correctness_datasets/Merged/tampered"

# Count the number of images in each category
originals_count = sum(len(files) for _, _, files in os.walk(merged_originals))
tampered_count = sum(len(files) for _, _, files in os.walk(merged_tampered))

print(f"‚úÖ Merged Originals: {originals_count} images")
print(f"‚úÖ Merged Tampered: {tampered_count} images")

import os
import cv2
import numpy as np
import random
from tqdm import tqdm

# Paths
merged_tampered = "/content/drive/MyDrive/image_correctness_datasets/Merged/tampered"
augmented_tampered = "/content/drive/MyDrive/image_correctness_datasets/Merged/tampered_augmented"

# Create folder for augmented images if not exists
os.makedirs(augmented_tampered, exist_ok=True)

# Augmentation functions
def random_flip(image):
    return cv2.flip(image, random.choice([-1, 0, 1]))  # Horizontal, vertical, both

def random_rotate(image):
    angle = random.randint(-30, 30)
    (h, w) = image.shape[:2]
    M = cv2.getRotationMatrix2D((w//2, h//2), angle, 1.0)
    return cv2.warpAffine(image, M, (w, h))

def random_brightness(image):
    factor = 0.5 + random.random()  # Random brightness change between 0.5x to 1.5x
    return np.clip(image * factor, 0, 255).astype(np.uint8)

def random_noise(image):
    noise = np.random.normal(0, 10, image.shape).astype(np.uint8)
    return cv2.add(image, noise)

# Process images
for filename in tqdm(os.listdir(merged_tampered)):
    file_path = os.path.join(merged_tampered, filename)
    image = cv2.imread(file_path)
    if image is None:
        continue

    # Generate augmented images
    for i in range(2):  # Create 2 new images per original
        aug_img = random.choice([random_flip, random_rotate, random_brightness, random_noise])(image)
        new_filename = f"{os.path.splitext(filename)[0]}_aug{i+1}.jpg"
        cv2.imwrite(os.path.join(augmented_tampered, new_filename), aug_img)

print("‚úÖ Augmentation Completed! Augmented images saved in", augmented_tampered)

import os
import shutil

# Paths
tampered_main = "/content/drive/MyDrive/image_correctness_datasets/Merged/tampered"
tampered_augmented = "/content/drive/MyDrive/image_correctness_datasets/Merged/tampered_augmented"

# Move augmented images to tampered folder
for filename in os.listdir(tampered_augmented):
    source = os.path.join(tampered_augmented, filename)
    destination = os.path.join(tampered_main, filename)

    # If a duplicate exists, rename the new file
    if os.path.exists(destination):
        base, ext = os.path.splitext(filename)
        new_filename = f"{base}_aug{ext}"
        destination = os.path.join(tampered_main, new_filename)

    shutil.move(source, destination)

# Remove the empty augmented folder
os.rmdir(tampered_augmented)

print("‚úÖ Augmented images merged successfully!")

import os

merged_path = "/content/drive/MyDrive/image_correctness_datasets/Merged"

originals = os.listdir(os.path.join(merged_path, "originals"))
tampered = os.listdir(os.path.join(merged_path, "tampered"))

print(f"‚úÖ Merged Originals: {len(originals)} images")
print(f"‚úÖ Merged Tampered: {len(tampered)} images")

import os
import cv2
import pandas as pd
from tqdm import tqdm

# Paths
merged_path = "/content/drive/MyDrive/image_correctness_datasets/Merged"
output_path = "/content/drive/MyDrive/image_correctness_datasets/Processed"

originals_dir = os.path.join(merged_path, "originals")
tampered_dir = os.path.join(merged_path, "tampered")
processed_dir = os.path.join(output_path, "images")

# Create processed directory
os.makedirs(processed_dir, exist_ok=True)

# CSV file to store labels
data = []

# Function to process images
def process_images(source_folder, label):
    for filename in tqdm(os.listdir(source_folder), desc=f"Processing {label} images"):
        file_path = os.path.join(source_folder, filename)

        # Read and resize image
        image = cv2.imread(file_path)
        if image is None:
            continue  # Skip unreadable images

        image = cv2.resize(image, (256, 256))  # Resize to 256x256

        # Save processed image
        processed_path = os.path.join(processed_dir, filename)
        cv2.imwrite(processed_path, image)

        # Store label (0 = original, 1 = tampered)
        data.append([processed_path, label])

# Process original (authentic) images
process_images(originals_dir, label=0)

# Process tampered (photoshopped) images
process_images(tampered_dir, label=1)

# Save CSV file
df = pd.DataFrame(data, columns=["image_path", "label"])
csv_path = os.path.join(output_path, "dataset_labels.csv")
df.to_csv(csv_path, index=False)

print(f"‚úÖ Preprocessing Completed! Processed images saved in {processed_dir}")
print(f"‚úÖ Labels CSV saved at {csv_path}")

import os
import shutil
import random

# Paths
dataset_path = "/content/drive/MyDrive/image_correctness_datasets/Merged"
train_dir = os.path.join(dataset_path, "train")
val_dir = os.path.join(dataset_path, "val")
test_dir = os.path.join(dataset_path, "test")

# Classes (original = correct, tampered = incorrect)
classes = ["originals", "tampered"]

# Create directories
for split in [train_dir, val_dir, test_dir]:
    for class_name in classes:
        os.makedirs(os.path.join(split, class_name), exist_ok=True)

# Split ratio
train_ratio = 0.7
val_ratio = 0.2
test_ratio = 0.1

# Function to split and move files
def split_data(source_folder, dest_folder, split_ratio):
    images = os.listdir(source_folder)
    random.shuffle(images)

    split_idx = int(len(images) * split_ratio)
    selected_images = images[:split_idx]

    for img in selected_images:
        src = os.path.join(source_folder, img)
        dst = os.path.join(dest_folder, img)
        shutil.move(src, dst)

# Apply split for both classes
for class_name in classes:
    class_path = os.path.join(dataset_path, class_name)

    split_data(class_path, os.path.join(train_dir, class_name), train_ratio)
    split_data(class_path, os.path.join(val_dir, class_name), val_ratio)
    split_data(class_path, os.path.join(test_dir, class_name), test_ratio)

print("‚úÖ Dataset split into Train, Validation, and Test sets successfully!")

import os
import tensorflow as tf
from tensorflow.keras.preprocessing.image import ImageDataGenerator
from tensorflow.keras.applications import EfficientNetB0
from tensorflow.keras.models import Model
from tensorflow.keras.layers import Dense, GlobalAveragePooling2D, Dropout
from tensorflow.keras.optimizers import Adam

# ‚úÖ Step 1: Define Dataset Paths
base_dir = "/content/drive/MyDrive/image_correctness_datasets/Merged"
train_dir = os.path.join(base_dir, "train")
val_dir = os.path.join(base_dir, "val")
test_dir = os.path.join(base_dir, "test")

# ‚úÖ Step 2: Define Image Data Generators (Augmentation)
IMG_SIZE = (224, 224)
BATCH_SIZE = 32

train_datagen = ImageDataGenerator(
    rescale=1.0 / 255,
    rotation_range=20,
    width_shift_range=0.2,
    height_shift_range=0.2,
    shear_range=0.2,
    zoom_range=0.2,
    horizontal_flip=True
)

val_test_datagen = ImageDataGenerator(rescale=1.0 / 255)

# ‚úÖ Step 3: Load Image Data
train_generator = train_datagen.flow_from_directory(
    train_dir, target_size=IMG_SIZE, batch_size=BATCH_SIZE, class_mode='binary'
)
val_generator = val_test_datagen.flow_from_directory(
    val_dir, target_size=IMG_SIZE, batch_size=BATCH_SIZE, class_mode='binary'
)
test_generator = val_test_datagen.flow_from_directory(
    test_dir, target_size=IMG_SIZE, batch_size=BATCH_SIZE, class_mode='binary'
)

# ‚úÖ Step 4: Load EfficientNetB0 Model (Without Top Layers)
base_model = EfficientNetB0(weights="imagenet", include_top=False, input_shape=(224, 224, 3))
base_model.trainable = False  # Freeze base model to retain pre-trained features

# ‚úÖ Step 5: Add Custom Layers
x = base_model.output
x = GlobalAveragePooling2D()(x)  # Convert feature maps to a single vector
x = Dense(128, activation="relu")(x)
x = Dropout(0.4)(x)  # Dropout to prevent overfitting
x = Dense(64, activation="relu")(x)
output_layer = Dense(1, activation="sigmoid")(x)  # Binary classification (Original vs. Tampered)

# ‚úÖ Step 6: Build the Model
model = Model(inputs=base_model.input, outputs=output_layer)

# ‚úÖ Step 7: Compile the Model
model.compile(optimizer=Adam(learning_rate=0.0001), loss="binary_crossentropy", metrics=["accuracy"])

# ‚úÖ Step 8: Train the Model
EPOCHS = 10

history = model.fit(
    train_generator,
    validation_data=val_generator,
    epochs=EPOCHS
)

# ‚úÖ Step 9: Evaluate on Test Data
test_loss, test_acc = model.evaluate(test_generator)
print(f"‚úÖ Test Accuracy: {test_acc * 100:.2f}%")

import tensorflow as tf
print("GPU Available:", tf.config.list_physical_devices('GPU'))

!pip install tensorflow tensorflow-addons --upgrade

import tensorflow as tf
from tensorflow import keras
from tensorflow.keras.models import Model
from tensorflow.keras.layers import Dense, Dropout, GlobalAveragePooling2D
from tensorflow.keras.applications import EfficientNetB0
from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau
from tensorflow.keras.preprocessing.image import ImageDataGenerator
from tensorflow.keras import mixed_precision

import os

# ‚úÖ Enable Mixed Precision (Faster Training)
mixed_precision.set_global_policy("mixed_float16")

# ‚úÖ Enable XLA (Accelerated Linear Algebra)
tf.config.optimizer.set_jit(True)

print("‚úÖ TensorFlow Version:", tf.__version__)
print("‚úÖ GPU:", tf.config.list_physical_devices("GPU"))

# Dataset Paths
BASE_DIR = "/content/drive/MyDrive/image_correctness_datasets/Merged"
TRAIN_DIR = os.path.join(BASE_DIR, "train")
VAL_DIR = os.path.join(BASE_DIR, "val")
TEST_DIR = os.path.join(BASE_DIR, "test")

# Image & Model Settings
IMG_SIZE = (224, 224)
BATCH_SIZE = 16  # Adjust based on memory
EPOCHS = 10
NUM_CLASSES = 1  # Binary classification

# Data Augmentation for Training
train_datagen = ImageDataGenerator(
    rescale=1./255,
    rotation_range=15,
    width_shift_range=0.1,
    height_shift_range=0.1,
    shear_range=0.1,
    zoom_range=0.1,
    horizontal_flip=True
)

val_test_datagen = ImageDataGenerator(rescale=1./255)

# Load Data Efficiently
train_generator = train_datagen.flow_from_directory(
    TRAIN_DIR, target_size=IMG_SIZE, batch_size=BATCH_SIZE, class_mode="binary"
)
val_generator = val_test_datagen.flow_from_directory(
    VAL_DIR, target_size=IMG_SIZE, batch_size=BATCH_SIZE, class_mode="binary"
)
test_generator = val_test_datagen.flow_from_directory(
    TEST_DIR, target_size=IMG_SIZE, batch_size=BATCH_SIZE, class_mode="binary"
)

# Load Pretrained EfficientNetB0 (without top layer)
base_model = EfficientNetB0(weights="imagenet", include_top=False, input_shape=(224, 224, 3))

# Freeze the base model (transfer learning)
base_model.trainable = False

# Custom Classification Head
x = GlobalAveragePooling2D()(base_model.output)
x = Dense(128, activation="relu")(x)
x = Dropout(0.3)(x)
output = Dense(1, activation="sigmoid", dtype=tf.float32)(x)  # Ensure correct dtype

# Create Model
model = Model(inputs=base_model.input, outputs=output)

# Compile Model
model.compile(
    optimizer=keras.optimizers.Adam(learning_rate=0.0001),
    loss="binary_crossentropy",
    metrics=["accuracy"]
)

model.summary()

# Callbacks
early_stopping = EarlyStopping(monitor="val_loss", patience=3, restore_best_weights=True)
reduce_lr = ReduceLROnPlateau(monitor="val_loss", factor=0.5, patience=2, verbose=1)

# Train Model
history = model.fit(
    train_generator,
    validation_data=val_generator,
    epochs=EPOCHS,
    callbacks=[early_stopping, reduce_lr]
)

# Evaluate on Test Set
test_loss, test_acc = model.evaluate(test_generator)
print(f"‚úÖ Test Accuracy: {test_acc:.2%}")

model.save("/content/drive/MyDrive/image_correctness_datasets/efficientnet_model.h5")
print("‚úÖ Model Saved Successfully!")

# Save the model in the new Keras format
model.save("/content/drive/MyDrive/image_correctness_datasets/efficientnet_model.keras")
print("‚úÖ Model Saved Successfully in Keras format!")

import tensorflow as tf

model_path = "/content/drive/MyDrive/image_correctness_datasets/efficientnet_model.keras"
model = tf.keras.models.load_model(model_path)

print("‚úÖ Model Loaded Successfully!")

import tensorflow as tf
print(tf.__version__)
# Load the .keras model
model_path = "/content/drive/MyDrive/image_correctness_datasets/efficientnet_model.keras"
model = tf.keras.models.load_model(model_path)

print("‚úÖ Model Loaded Successfully!")

# Define the directory to save the converted model in SavedModel format
saved_model_path = "/content/drive/MyDrive/image_correctness_datasets/finalefficientnet_saved"

# Define input signature
@tf.function(input_signature=[tf.TensorSpec(shape=[None, 224, 224, 3], dtype=tf.float32)])
def serving_input_fn(input_tensor):
    return model(input_tensor)

# Save the model in SavedModel format
tf.saved_model.save(model, saved_model_path, signatures={'serving_default': serving_input_fn})

print(f"‚úÖ Model saved in TensorFlow SavedModel format at: {saved_model_path}")

model.export("/content/drive/MyDrive/image_correctness_datasets/tf_model")

!ls "/content/drive/MyDrive/image_correctness_datasets/tf_model"

from google.colab import files
files.download("/content/drive/MyDrive/image_correctness_datasets/tf_model/saved_model.pb")

import tensorflow as tf

model_path = "/content/drive/MyDrive/image_correctness_datasets/tf_model/"  # Adjust if needed
model = tf.saved_model.load(model_path)

print("Available Signatures:")
for signature_key in model.signatures:
    print(f"üîπ Signature: {signature_key}")
    signature = model.signatures[signature_key]
    print("üîπ Inputs:", signature.structured_input_signature)
    print("üîπ Outputs:", signature.structured_outputs)

# Commented out IPython magic to ensure Python compatibility.
# %pip install tensorflow==2.18.0
import tensorflow as tf
print(tf.__version__)  # Should print 2.12.0

import tensorflow as tf
print(tf.__version__)

# Step 2: Load the existing model (adjust path if needed)
model_path = "/content/drive/MyDrive/image_correctness_datasets/tf_model"  # Adjust if needed
model = tf.saved_model.load(model_path)

# Step 3: Save the model in TensorFlow 2.12 format
converted_model_path = "/content/drive/MyDrive/image_correctness_datasets/"
tf.saved_model.save(model, converted_model_path)

print(f"Model successfully converted and saved to {converted_model_path}")

from google.colab import drive
drive.mount('/content/drive')

# prompt: give me code for finding available signatures of uploaded file in .pb  this is a model  and path is /saved_model.pb

import tensorflow as tf

model_path = "/content/drive/MyDrive/image_correctness_datasets/tf_model"  # Replace with the actual path to your .pb file
model = tf.saved_model.load(model_path)

print("Available Signatures:")
for signature_key in model.signatures:
    print(f"  Signature: {signature_key}")
    signature = model.signatures[signature_key]
    print(f"    Inputs: {signature.structured_input_signature}")
    print(f"    Outputs: {signature.structured_outputs}")

import tensorflow as tf

# Load the TensorFlow 2.18 model
model_path = "/content/drive/MyDrive/image_correctness_datasets/efficientnet_model_saved"
model = tf.saved_model.load(model_path)

print("‚úÖ Model Loaded Successfully!")

# Make the model compatible with TensorFlow 1.x
tf.compat.v1.disable_eager_execution()  # Disable eager execution to use TensorFlow 1.x compatibility mode

# Create a TensorFlow 1.x session and save the model
with tf.compat.v1.Session() as sess:
    tf.compat.v1.saved_model.simple_save(
        sess,
        "/content/drive/MyDrive/image_correctness_datasets/efficientnet_model_tf1",  # The path to save the TensorFlow 1.x model
        inputs={"input_layer": model.signatures["serving_default"].inputs[0]},  # Specify the input layer
        outputs={"output_0": model.signatures["serving_default"].outputs[0]}  # Specify the output layer
    )

print("‚úÖ Model converted to TensorFlow 1.15 format!")

import shutil

# Path to the folder you want to zip
folder_path = '/content/drive/MyDrive/image_correctness_datasets/efficientnet_model_tf1'
# Path to save the zip file
zip_path = '/content/drive/MyDrive/image_correctness_datasets/efficientnet_model_tf1.zip'

# Zip the folder
shutil.make_archive(zip_path.replace('.zip', ''), 'zip', folder_path)

print(f"‚úÖ Folder zipped and saved as {zip_path}")

!mv "/content/drive/MyDrive/Colab Notebooks/image_correctness_datasets" "/content/drive/MyDrive/"